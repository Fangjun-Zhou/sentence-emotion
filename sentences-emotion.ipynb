{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORD_EMBEDDING_MODEL_PATH = \"models/skipgram_emotion_text_transfer/best_val_model_5.98.pt\"\n",
    "VOCAB_PATH = \"models/skipgram_emotion_text_transfer/vocab.pt\"\n",
    "\n",
    "DATA_SET_SIZE = 3000\n",
    "BATCH_SIZE = 64\n",
    "LEARING_RATE = 0.01\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fangjunzhou/Documents/ML_Project/sentence-emotion/venv/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import random_split\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "# from model import SentenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try to use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i didnt feel humiliated</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i can go from feeling so hopeless to so damned...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im grabbing a minute to post i feel greedy wrong</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i am ever feeling nostalgic about the fireplac...</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i am feeling grouchy</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Emotion\n",
       "0                            i didnt feel humiliated  sadness\n",
       "1  i can go from feeling so hopeless to so damned...  sadness\n",
       "2   im grabbing a minute to post i feel greedy wrong    anger\n",
       "3  i am ever feeling nostalgic about the fireplac...     love\n",
       "4                               i am feeling grouchy    anger"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the emotions dataset.\n",
    "sentenceDf = pd.read_csv(\"data/text-emotion.zip\")\n",
    "sentenceDf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i didnt feel humiliated'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample the first 5 sentences.\n",
    "sentence = sentenceDf[\"Text\"][0]\n",
    "sentence"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Word Embedding Model and Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = torch.load(WORD_EMBEDDING_MODEL_PATH, map_location=device)\n",
    "# Get the weight of the embedding layer.\n",
    "embedding_weight = embedding_model[\"embeddings.weight\"]\n",
    "vocab = torch.load(VOCAB_PATH, map_location=device)\n",
    "tokenizer = get_tokenizer(\"basic_english\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['anger', 'fear', 'happy', 'love', 'sadness', 'surprise'],\n",
       "       dtype=object)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One-hot encode the labels.\n",
    "emotion_encoder = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "emotion_encoder.fit(sentenceDf[[\"Emotion\"]].values)\n",
    "emotion_encoder.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emotion Dataset.\n",
    "class EmotionDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        vocab,\n",
    "        tokenizer,\n",
    "        emotion_encoder: OneHotEncoder,\n",
    "        max_length=20,\n",
    "        size=-1\n",
    "    ):\n",
    "        self.df = df\n",
    "        if size > 0:\n",
    "            self.df = self.df.sample(size).reset_index(drop=True)\n",
    "        self.vocab = vocab\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.emotion_encoder = emotion_encoder\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.df[\"Text\"][idx]\n",
    "        emotion = self.df[\"Emotion\"][idx]\n",
    "        # Convert the sentence to a list of tokens.\n",
    "        tokens = self.tokenizer(sentence)\n",
    "        # Convert the tokens to indices.\n",
    "        indices = [self.vocab[token] for token in tokens]\n",
    "        # Crop the indices to the max length.\n",
    "        if len(indices) > self.max_length:\n",
    "            indices = indices[:self.max_length]\n",
    "        # Padding the indices to the max length.\n",
    "        if len(indices) < self.max_length:\n",
    "            indices = indices + [0] * (self.max_length - len(indices))\n",
    "        # Convert the indices to a tensor.\n",
    "        indices = torch.tensor(indices)\n",
    "        # One-hot encode the label.\n",
    "        label = self.emotion_encoder.transform([[emotion]]).toarray()\n",
    "        # Convert the label to a tensor.\n",
    "        label = torch.tensor(label, dtype=torch.float32)\n",
    "        return indices, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800, 200)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate the emotion dataset.\n",
    "dataset = EmotionDataset(sentenceDf, vocab, tokenizer, emotion_encoder, max_length=20, size=DATA_SET_SIZE)\n",
    "# Split the dataset into train and validation.\n",
    "train_set, val_set = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "len(train_set), len(val_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_data_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM model to classify the sentiment of a given sentence\n",
    "class SentenceModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding: nn.Embedding,\n",
    "        input_word_num,\n",
    "        hidden_dim,\n",
    "        output_dim,\n",
    "        n_layers\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding = embedding\n",
    "        self.embedding_dim = embedding.embedding_dim\n",
    "        self.lstm = nn.LSTM(\n",
    "            self.embedding_dim * input_word_num,\n",
    "            hidden_dim,\n",
    "            num_layers=n_layers,\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text)\n",
    "        # print(f\"Embedded shape: {embedded.shape}\")\n",
    "        # Read the documentation.\n",
    "        for i in range(0, embedded.shape[1]-1):\n",
    "            input_word = embedded[:, i:i+2, :]\n",
    "            input_word = input_word.view(input_word.shape[0], -1)\n",
    "            # print(f\"Input word shape: {input_word.shape}\")\n",
    "            if i == 0:\n",
    "                output, (hidden, cell) = self.lstm(input_word)\n",
    "            else:\n",
    "                output, (hidden, cell) = self.lstm(input_word, (hidden, cell))\n",
    "        \n",
    "        x = self.fc(output.squeeze(0))\n",
    "        x = F.softmax(x, dim=1)\n",
    "        x = x.view(x.shape[0], 1, -1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceModel(\n",
       "  (embedding): Embedding(6826, 300)\n",
       "  (lstm): LSTM(600, 128, num_layers=2)\n",
       "  (fc): Linear(in_features=128, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model.\n",
    "model = SentenceModel(\n",
    "    embedding=nn.Embedding.from_pretrained(embedding_weight),\n",
    "    input_word_num=2,\n",
    "    hidden_dim=128,\n",
    "    output_dim=emotion_encoder.categories_[0].shape[0],\n",
    "    n_layers=2\n",
    ")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify optimizer and loss function.\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARING_RATE)\n",
    "loss_function = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10\n",
      "Training Loss: 0.434757\n",
      "Validation Loss: 0.413351\n",
      "Epoch: 2/10\n",
      "Training Loss: 0.408804\n",
      "Validation Loss: 0.411448\n",
      "Epoch: 3/10\n",
      "Training Loss: 0.406781\n",
      "Validation Loss: 0.410742\n",
      "Epoch: 4/10\n",
      "Training Loss: 0.405024\n",
      "Validation Loss: 0.409863\n",
      "Epoch: 5/10\n",
      "Training Loss: 0.396665\n",
      "Validation Loss: 0.421414\n",
      "Epoch: 6/10\n",
      "Training Loss: 0.384015\n",
      "Validation Loss: 0.418144\n",
      "Epoch: 7/10\n",
      "Training Loss: 0.368931\n",
      "Validation Loss: 0.432806\n",
      "Epoch: 8/10\n",
      "Training Loss: 0.351597\n",
      "Validation Loss: 0.444981\n",
      "Epoch: 9/10\n",
      "Training Loss: 0.343085\n",
      "Validation Loss: 0.462324\n",
      "Epoch: 10/10\n",
      "Training Loss: 0.331921\n",
      "Validation Loss: 0.483819\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "val_loss = []\n",
    "# Train the model.\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss = 0.0\n",
    "    val_loss = 0.0\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        # Get the input and label.\n",
    "        input = batch[0].to(device)\n",
    "        label = batch[1].to(device)\n",
    "        # Reset the gradients.\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass.\n",
    "        output = model(input)\n",
    "        # Calculate the loss.\n",
    "        loss = loss_function(output, label)\n",
    "        # Backward pass.\n",
    "        loss.backward()\n",
    "        # Update the weights.\n",
    "        optimizer.step()\n",
    "        # Accumulate the loss.\n",
    "        train_loss += loss.item() * input.size(0)\n",
    "\n",
    "    # Evaluate the model.\n",
    "    model.eval()\n",
    "    for batch in val_data_loader:\n",
    "        # Get the input and label.\n",
    "        input = batch[0].to(device)\n",
    "        label = batch[1].to(device)\n",
    "        # Forward pass.\n",
    "        output = model(input)\n",
    "        # Calculate the loss.\n",
    "        loss = loss_function(output, label)\n",
    "        # Accumulate the loss.\n",
    "        val_loss += loss.item() * input.size(0)\n",
    "\n",
    "    # Calculate the average losses.\n",
    "    train_loss = train_loss / len(train_dataloader.dataset)\n",
    "    val_loss = val_loss / len(val_data_loader.dataset)\n",
    "\n",
    "    # Print the progress.\n",
    "    print(f\"Epoch: {epoch+1}/{EPOCHS}\")\n",
    "    print(f\"Training Loss: {train_loss:.6f}\")\n",
    "    print(f\"Validation Loss: {val_loss:.6f}\")\n",
    "    train_losses.append(train_loss)\n",
    "    val_loss.append(val_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0fe8bba7e76fdea859f6d15572056f4b104ffcaa7fcaf0bd2c03ce90fac10810"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
