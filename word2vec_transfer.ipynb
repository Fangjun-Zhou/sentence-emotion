{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add word2vec to the python path.\n",
    "import sys\n",
    "sys.path.append(\"external/word2vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fangj/sentence-emotion/venv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "import yaml\n",
    "import zipfile\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchtext.vocab import Vocab\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "from external.word2vec.train import train\n",
    "from external.word2vec.utils.helper import (\n",
    "    get_model_class,\n",
    "    get_optimizer_class,\n",
    "    get_lr_scheduler,\n",
    "    save_vocab,\n",
    "    load_vocab,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_PATH = \"config.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(CONFIG_PATH, \"r\") as f:\n",
    "    config = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "\n",
    "The corpus used for this training is [Twitter Financial News](https://www.kaggle.com/datasets/sulphatet/twitter-financial-news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42918"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the emotion text data.\n",
    "emotion_df = pd.read_csv(\"data/text-emotion.zip\")\n",
    "emotion_df.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset for twitter financial news text.\n",
    "class EmotionTextDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, size = -1):\n",
    "        self.emotion_text = df\n",
    "        # Shuffle and take a subset of the data.\n",
    "        if size > 0:\n",
    "            self.emotion_text = self.emotion_text.sample(frac=1).reset_index(drop=True)\n",
    "            self.emotion_text = self.emotion_text[:size]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.emotion_text)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.emotion_text.iloc[idx, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                              i didnt feel humiliated\n",
       "1    i can go from feeling so hopeless to so damned...\n",
       "2     im grabbing a minute to post i feel greedy wrong\n",
       "3    i am ever feeling nostalgic about the fireplac...\n",
       "4                                 i am feeling grouchy\n",
       "5    ive been feeling a little burdened lately wasn...\n",
       "6    ive been taking or milligrams or times recomme...\n",
       "7    i feel as confused about life as a teenager or...\n",
       "8    i have been with petronas for years i feel tha...\n",
       "9                                  i feel romantic too\n",
       "Name: Text, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the datset.\n",
    "emotion_dataset = EmotionTextDataset(emotion_df, size = config[\"dataset_size\"])\n",
    "emotion_dataset[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained vocab size: 6630\n"
     ]
    }
   ],
   "source": [
    "if (config[\"pre_trained_vocab_path\"]):\n",
    "    vocab: Vocab = load_vocab(config[\"pre_trained_vocab_path\"])\n",
    "    vocab_size = len(vocab.get_stoi())\n",
    "    print(f\"Pretrained vocab size: {vocab_size}\")\n",
    "else:\n",
    "    vocab = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "870"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the english tokenizer.\n",
    "tokenizer = get_tokenizer(\"basic_english\", language=\"en\")\n",
    "# Build the extended vocab based on dataset.\n",
    "extend_vocab = build_vocab_from_iterator(\n",
    "    map(tokenizer, emotion_dataset),\n",
    "    specials=[\"<unk>\"],\n",
    "    min_freq=config[\"vocab_min_word_frequency\"]\n",
    ")\n",
    "extend_vocab.set_default_index(extend_vocab[\"<unk>\"])\n",
    "len(extend_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86 new tokens added to the vocab.\n",
      "Extended vocab size: 6716\n"
     ]
    }
   ],
   "source": [
    "if vocab:\n",
    "    new_token = []\n",
    "    for word in extend_vocab.get_stoi():\n",
    "        if not word in vocab:\n",
    "            new_token.append(word)\n",
    "    # Add all new tokens to the vocab.\n",
    "    for token in new_token:\n",
    "        vocab.append_token(token)\n",
    "    print(f\"{len(new_token)} new tokens added to the vocab.\")\n",
    "    vocab_size = len(vocab.get_stoi())\n",
    "    print(f\"Extended vocab size: {vocab_size}\")\n",
    "else:\n",
    "    vocab = extend_vocab\n",
    "    vocab_size = len(vocab.get_stoi())\n",
    "    print(f\"Extended vocab size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the pretrained model.\n",
    "if config[\"pre_trained_model_path\"]:\n",
    "    pretrained_model = torch.load(config[\"pre_trained_model_path\"], map_location=torch.device(\"cpu\"))\n",
    "else:\n",
    "    pretrained_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Vocabulary size: 6716\n",
      "Adjusting learning rate of group 0 to 2.5000e-02.\n",
      "Transfer learning enabled. Pre-trained model loaded.\n",
      "Epoch: 1/16, Train Loss=6.19652, Val Loss=6.05199\n",
      "Time elapsed: 0.16 min, average epoch time: 0.16 min, predicting finish time: 2.48 min\n",
      "Adjusting learning rate of group 0 to 2.3438e-02.\n",
      "Epoch: 2/16, Train Loss=5.96399, Val Loss=6.00583\n",
      "Time elapsed: 0.30 min, average epoch time: 0.15 min, predicting finish time: 2.43 min\n",
      "Adjusting learning rate of group 0 to 2.1875e-02.\n",
      "Epoch: 3/16, Train Loss=5.90004, Val Loss=5.96273\n",
      "Time elapsed: 0.46 min, average epoch time: 0.15 min, predicting finish time: 2.44 min\n",
      "Adjusting learning rate of group 0 to 2.0313e-02.\n",
      "Epoch: 4/16, Train Loss=5.84412, Val Loss=5.94971\n",
      "Time elapsed: 0.61 min, average epoch time: 0.15 min, predicting finish time: 2.45 min\n",
      "Adjusting learning rate of group 0 to 1.8750e-02.\n",
      "Epoch: 5/16, Train Loss=5.80430, Val Loss=5.94428\n",
      "Time elapsed: 0.76 min, average epoch time: 0.15 min, predicting finish time: 2.44 min\n",
      "Adjusting learning rate of group 0 to 1.7188e-02.\n",
      "Epoch: 6/16, Train Loss=5.77639, Val Loss=5.93859\n",
      "Time elapsed: 0.91 min, average epoch time: 0.15 min, predicting finish time: 2.43 min\n",
      "Adjusting learning rate of group 0 to 1.5625e-02.\n",
      "Epoch: 7/16, Train Loss=5.74772, Val Loss=5.93374\n",
      "Time elapsed: 1.06 min, average epoch time: 0.15 min, predicting finish time: 2.42 min\n",
      "Adjusting learning rate of group 0 to 1.4063e-02.\n",
      "Epoch: 8/16, Train Loss=5.72646, Val Loss=5.93774\n",
      "Time elapsed: 1.20 min, average epoch time: 0.15 min, predicting finish time: 2.41 min\n",
      "Adjusting learning rate of group 0 to 1.2500e-02.\n",
      "Epoch: 9/16, Train Loss=5.70331, Val Loss=5.93924\n",
      "Time elapsed: 1.35 min, average epoch time: 0.15 min, predicting finish time: 2.40 min\n",
      "Adjusting learning rate of group 0 to 1.0938e-02.\n",
      "Epoch: 10/16, Train Loss=5.68320, Val Loss=5.93938\n",
      "Time elapsed: 1.50 min, average epoch time: 0.15 min, predicting finish time: 2.39 min\n",
      "Adjusting learning rate of group 0 to 9.3750e-03.\n",
      "Epoch: 11/16, Train Loss=5.65847, Val Loss=5.94097\n",
      "Time elapsed: 1.64 min, average epoch time: 0.15 min, predicting finish time: 2.39 min\n",
      "Adjusting learning rate of group 0 to 7.8125e-03.\n",
      "Epoch: 12/16, Train Loss=5.63640, Val Loss=5.93910\n",
      "Time elapsed: 1.79 min, average epoch time: 0.15 min, predicting finish time: 2.39 min\n",
      "Adjusting learning rate of group 0 to 6.2500e-03.\n",
      "Epoch: 13/16, Train Loss=5.60567, Val Loss=5.93658\n",
      "Time elapsed: 1.94 min, average epoch time: 0.15 min, predicting finish time: 2.39 min\n",
      "Adjusting learning rate of group 0 to 4.6875e-03.\n",
      "Epoch: 14/16, Train Loss=5.57556, Val Loss=5.93766\n",
      "Time elapsed: 2.09 min, average epoch time: 0.15 min, predicting finish time: 2.39 min\n",
      "Adjusting learning rate of group 0 to 3.1250e-03.\n",
      "Epoch: 15/16, Train Loss=5.53828, Val Loss=5.93313\n",
      "Time elapsed: 2.24 min, average epoch time: 0.15 min, predicting finish time: 2.39 min\n",
      "Adjusting learning rate of group 0 to 1.5625e-03.\n",
      "Epoch: 16/16, Train Loss=5.49821, Val Loss=5.93180\n",
      "Time elapsed: 2.39 min, average epoch time: 0.15 min, predicting finish time: 2.39 min\n",
      "Adjusting learning rate of group 0 to 0.0000e+00.\n",
      "Training finished.\n",
      "Model artifacts saved to folder: models/skipgram_emotion_text_blog_transfer\n"
     ]
    }
   ],
   "source": [
    "if pretrained_model:\n",
    "    train(\n",
    "        config=config,\n",
    "        data_iter=emotion_dataset,\n",
    "        vocab=vocab,\n",
    "        transfer_model=pretrained_model\n",
    "    )\n",
    "else:\n",
    "    train(\n",
    "        config=config,\n",
    "        data_iter=emotion_dataset,\n",
    "        vocab=vocab\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "68bbf37daed5712175a3ee6d43d70d965f6c7f52673c19145143d1430c8dd7b3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
