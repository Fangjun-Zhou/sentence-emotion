{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add word2vec to the python path.\n",
    "import sys\n",
    "sys.path.append(\"external/word2vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import yaml\n",
    "import zipfile\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchtext.vocab import Vocab\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "from external.word2vec.train import train\n",
    "from external.word2vec.utils.helper import (\n",
    "    get_model_class,\n",
    "    get_optimizer_class,\n",
    "    get_lr_scheduler,\n",
    "    save_vocab,\n",
    "    load_vocab,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_PATH = \"config.yaml\"\n",
    "\n",
    "DATA_SET_SIZE = -1\n",
    "\n",
    "PRE_TRAINED_MODEL_PATH = os.path.join(*[\"external\", \"word2vec\", \"models\", \"skipgram_blog\", \"best_val_model_5.67.pt\"])\n",
    "PRE_TRAINED_VOCAB_PATH = os.path.join(*[\"external\", \"word2vec\", \"models\", \"skipgram_blog\", \"vocab.pt\"])\n",
    "# PRE_TRAINED_MODEL_PATH = None\n",
    "# PRE_TRAINED_VOCAB_PATH = None\n",
    "\n",
    "VOCAB_MIN_WORD_FREQUENCY = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(CONFIG_PATH, \"r\") as f:\n",
    "    config = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "\n",
    "The corpus used for this training is [Twitter Financial News](https://www.kaggle.com/datasets/sulphatet/twitter-financial-news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the emotion text data.\n",
    "emotion_df = pd.read_csv(\"data/text-emotion.zip\")\n",
    "emotion_df.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset for twitter financial news text.\n",
    "class EmotionTextDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, size = -1):\n",
    "        self.emotion_text = df\n",
    "        # Shuffle and take a subset of the data.\n",
    "        if size > 0:\n",
    "            self.emotion_text = self.emotion_text.sample(frac=1).reset_index(drop=True)\n",
    "            self.emotion_text = self.emotion_text[:size]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.emotion_text)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.emotion_text.iloc[idx, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the datset.\n",
    "emotion_dataset = EmotionTextDataset(emotion_df, size = DATA_SET_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (PRE_TRAINED_VOCAB_PATH):\n",
    "    vocab: Vocab = load_vocab(PRE_TRAINED_VOCAB_PATH)\n",
    "    vocab_size = len(vocab.get_stoi())\n",
    "    print(f\"Pretrained vocab size: {vocab_size}\")\n",
    "else:\n",
    "    vocab = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the english tokenizer.\n",
    "tokenizer = get_tokenizer(\"basic_english\", language=\"en\")\n",
    "# Build the extended vocab based on dataset.\n",
    "extend_vocab = build_vocab_from_iterator(\n",
    "    map(tokenizer, emotion_dataset),\n",
    "    min_freq=VOCAB_MIN_WORD_FREQUENCY\n",
    ")\n",
    "len(extend_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_token = []\n",
    "for word in extend_vocab.get_stoi():\n",
    "    if not word in vocab:\n",
    "        new_token.append(word)\n",
    "# Add all new tokens to the vocab.\n",
    "for token in new_token:\n",
    "    vocab.append_token(token)\n",
    "print(f\"{len(new_token)} new tokens added to the vocab.\")\n",
    "vocab_size = len(vocab.get_stoi())\n",
    "print(f\"Extended vocab size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the pretrained model.\n",
    "pretrained_model = torch.load(PRE_TRAINED_MODEL_PATH, map_location=torch.device(\"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(\n",
    "    config=config,\n",
    "    data_iter=emotion_dataset,\n",
    "    vocab=vocab,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0fe8bba7e76fdea859f6d15572056f4b104ffcaa7fcaf0bd2c03ce90fac10810"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
